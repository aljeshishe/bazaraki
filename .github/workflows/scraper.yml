name: Bazaraki Scraper

on:
  # schedule:
  #   # Runs every 5 minutes
  #   - cron: '*/5 * * * *'
  workflow_dispatch: # Allows manual triggering

jobs:
  scrape:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install Poetry
      uses: snok/install-poetry@v1
      with:
        version: 1.8.2
        virtualenvs-create: true
        virtualenvs-in-project: true
        
    - name: Cache Poetry dependencies
      uses: actions/cache@v3
      with:
        path: .venv
        key: ${{ runner.os }}-poetry-${{ hashFiles('**/poetry.lock') }}
        restore-keys: |
          ${{ runner.os }}-poetry-
          
    - name: Install dependencies
      run: |
        poetry install --no-dev
        
    - name: Run scraper
      run: |
        poetry run scrapy crawl property_spider -a  urls=https://www.bazaraki.com/real-estate-to-rent,https://www.bazaraki.com/real-estate-for-sale
    # poetry run scrapy crawl property_spider -a fast=1 -a urls=https://www.bazaraki.com/real-estate-to-rent,https://www.bazaraki.com/real-estate-for-sale
        
    # - name: Configure AWS credentials
    #   if: success()
    #   uses: aws-actions/configure-aws-credentials@v4
    #   with:
    #     aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
    #     aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
    #     aws-region: us-east-1  # Change this to your preferred region
        
    # - name: Sync to S3
    #   if: success()
    #   run: |
    #     poetry run aws s3 sync output s3://ab-users/grachev/bazaraki/output
        
    # - name: Upload artifacts on failure
    #   if: failure()
    #   uses: actions/upload-artifact@v3
    #   with:
    #     name: scraper-logs
    #     path: |
    #       **/*.log
    #       output/
    #     retention-days: 7